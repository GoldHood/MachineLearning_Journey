
# ‚ú® An√°lisis y Clasificaci√≥n del Clima en el Aeropuerto Jorge Ch√°vez ‚ú®

<p align="center">
  <strong>Un estudio integral para predecir el clima utilizando t√©cnicas de Machine Learning.</strong>
</p>

---

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg?style=for-the-badge&logo=python)](https://www.python.org/)
[![MIT License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)
[![Open Source Love](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-red.svg?style=for-the-badge)]()
[![Contributions Welcome](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg?style=for-the-badge&logo=github)]()
[![Status](https://img.shields.io/badge/Status-Active-success.svg?style=for-the-badge)]()

---

## üìö **√çndice**

1. [Descripci√≥n General del Dataset](#descripci√≥n-general-del-dataset)
2. [Fundamento Te√≥rico](#fundamento-te√≥rico)
   - [√Årboles de Decisi√≥n](#1-√°rboles-de-decisi√≥n)
   - [Random Forest](#2-random-forest)
   - [Gradient Boosting](#3-gradient-boosting)
   - [Matriz de Confusi√≥n](#4-matriz-de-confusi√≥n)
3. [An√°lisis Exploratorio y Preprocesamiento](#an√°lisis-exploratorio-y-preprocesamiento)
   - [Carga del Dataset](#carga-del-dataset)
   - [Estad√≠sticas Descriptivas y Valores Nulos](#estad√≠sticas-descriptivas-y-valores-nulos)
   - [Eliminaci√≥n de Columnas Irrelevantes](#eliminaci√≥n-de-columnas-irrelevantes)
   - [Conversi√≥n de Variables Categ√≥ricas](#conversi√≥n-de-variables-categ√≥ricas)
   - [Tratamiento de Outliers](#tratamiento-de-outliers)
   - [Definici√≥n de Variable Objetivo y Caracter√≠sticas](#definici√≥n-de-variable-objetivo-y-caracter√≠sticas)
4. [Visualizaci√≥n de la Matriz de Correlaci√≥n](#visualizaci√≥n-de-la-matriz-de-correlaci√≥n)
5. [Entrenamiento de Modelos](#entrenamiento-de-modelos)
   - [√Årbol de Decisi√≥n](#√°rbol-de-decisi√≥n)
   - [Random Forest](#random-forest)
   - [Gradient Boosting](#gradient-boosting)
   - [Comparaci√≥n de Modelos y Validaci√≥n Cruzada](#comparaci√≥n-de-modelos-y-validaci√≥n-cruzada)
6. [Importancia de Caracter√≠sticas](#importancia-de-caracter√≠sticas)
7. [Conclusiones y Recomendaciones](#conclusiones-y-recomendaciones)

---

## üîç **Descripci√≥n General del Dataset**

El dataset utilizado contiene **3480 registros** recopilados en el Aeropuerto Internacional Jorge Ch√°vez (Lima, Per√∫), con 13 columnas que representan variables meteorol√≥gicas y la clasificaci√≥n del clima del d√≠a actual y del d√≠a siguiente. 

### Variables Principales:
- `maxC`: Temperatura m√°xima (¬∞C).
- `minC`: Temperatura m√≠nima (¬∞C).
- `rocioC`: Punto de roc√≠o (¬∞C).
- `hum_rel`: Humedad relativa (%).
- `nubosidad_por`: Porcentaje de nubosidad.
- `clase_hoy`: Clase del clima actual.
- `clase_manana`: Clase del clima del d√≠a siguiente (variable objetivo).

El objetivo es predecir `clase_manana` bas√°ndonos en las variables mencionadas utilizando modelos de Machine Learning.

---

## üîó **Fundamento Te√≥rico**

### 1. √Årboles de Decisi√≥n
Un √°rbol de decisi√≥n es un modelo de clasificaci√≥n que divide recursivamente el espacio de datos en regiones homog√©neas respecto a la variable objetivo. Se construye seleccionando las caracter√≠sticas que generan la mejor partici√≥n en cada nodo, utilizando m√©tricas como entrop√≠a o Gini.

#### Ventajas:
- Interpretaci√≥n simple y visualizaci√≥n intuitiva.
- Manejo efectivo de datos cualitativos y cuantitativos.

#### Desventajas:
- Propenso al sobreajuste si no se controla la profundidad del √°rbol.
- Menor precisi√≥n en problemas complejos con alta dimensionalidad.

#### Ejemplo de C√≥digo:
```python
from sklearn.tree import DecisionTreeClassifier

model_dt = DecisionTreeClassifier(random_state=42)
model_dt.fit(X_train, y_train)
y_pred_dt = model_dt.predict(X_test)
```
#### Matriz de Confusi√≥n para √Årbol de Decisi√≥n
```python
from sklearn.metrics import confusion_matrix, classification_report

cm_dt = confusion_matrix(y_test, y_pred_dt)
print("Matriz de Confusi√≥n para √Årbol de Decisi√≥n:")
print(cm_dt)
print(classification_report(y_test, y_pred_dt))
```
**Resultados:**
- **Exactitud:** 64%.
- **Observaciones:** La matriz mostr√≥ que las confusiones se concentraron entre clases similares, como `nublado` y `parcialmente nublado`, indicando que el modelo no logr√≥ distinguir patrones finos.

#### An√°lisis de Resultados:
- El √°rbol tiende a sobreajustarse cuando la profundidad no est√° limitada. Por ello, controlar este par√°metro es crucial para mejorar el desempe√±o en datos no vistos.

---

### 2. Random Forest
Random Forest combina m√∫ltiples √°rboles de decisi√≥n para reducir la varianza y mejorar la robustez del modelo.

#### Ventajas:
- Alta capacidad predictiva.
- Menor riesgo de sobreajuste debido al uso de muestras bootstrap y subconjuntos aleatorios de caracter√≠sticas.

#### Desventajas:
- Mayor complejidad computacional.
- Dificultad para interpretar resultados individuales.

#### Ejemplo de C√≥digo:
```python
from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(random_state=42)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
```
#### Matriz de Confusi√≥n para Random Forest
```python
cm_rf = confusion_matrix(y_test, y_pred_rf)
print("Matriz de Confusi√≥n para Random Forest:")
print(cm_rf)
print(classification_report(y_test, y_pred_rf))
```
**Resultados:**
- **Exactitud:** 71%.
- **Observaciones:** Se redujeron las confusiones en comparaci√≥n con el √Årbol de Decisi√≥n. El modelo mostr√≥ una mejor capacidad para distinguir entre clases.

#### An√°lisis de Resultados:
- Random Forest es m√°s estable debido a su enfoque en la aleatorizaci√≥n. La votaci√≥n mayoritaria entre los √°rboles mejora la precisi√≥n general del modelo.

---

### 3. Gradient Boosting
Gradient Boosting optimiza secuencialmente los errores de modelos previos mediante ajustes incrementales en cada iteraci√≥n.

#### Ventajas:
- Excelente rendimiento en datos desbalanceados y complejos.
- Alta capacidad para identificar patrones no lineales.

#### Desventajas:
- Entrenamiento lento debido a su naturaleza secuencial.
- Sensible a la elecci√≥n de hiperpar√°metros.

#### Ejemplo de C√≥digo:
```python
from sklearn.ensemble import GradientBoostingClassifier

model_gb = GradientBoostingClassifier(random_state=42)
model_gb.fit(X_train, y_train)
y_pred_gb = model_gb.predict(X_test)
```
#### Matriz de Confusi√≥n para Gradient Boosting
```python
cm_gb = confusion_matrix(y_test, y_pred_gb)
print("Matriz de Confusi√≥n para Gradient Boosting:")
print(cm_gb)
print(classification_report(y_test, y_pred_gb))
```
**Resultados:**
- **Exactitud:** 72%.
- **Observaciones:** Gradient Boosting present√≥ el mejor desempe√±o general, con menos errores en todas las clases.

#### An√°lisis de Resultados:
- El modelo logr√≥ captar interacciones complejas entre variables, lo que le permiti√≥ superar a los dem√°s algoritmos en t√©rminos de exactitud y balance.

---

## üìä **An√°lisis Exploratorio y Preprocesamiento**

### Carga del Dataset
```python
import pandas as pd

df = pd.read_csv('clima_aeropuerto_lima.csv', encoding='latin-1')
print(df.head())
```
- **Explicaci√≥n:** Este paso inicial asegura que los datos se carguen correctamente y que no existan problemas de codificaci√≥n. Revisar las primeras filas permite entender la estructura del dataset.

---

### Estad√≠sticas Descriptivas y Valores Nulos
```python
print(df.describe())
print(df.info())
print(df.isnull().sum())
```
- **Resultados:**
  - Confirmaci√≥n de que no hay valores nulos.
  - Las estad√≠sticas muestran que las variables tienen rangos coherentes.

---

### Tratamiento de Outliers
```python
def remove_outliers(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower) & (df[col] <= upper)]
    return df

columns_to_clean = ['maxC', 'minC', 'rocioC', 'hum_rel', 'nubosidad_por']
df = remove_outliers(df, columns_to_clean)
print(df.shape)
```
- **Explicaci√≥n:** Los outliers fueron identificados usando el rango intercuart√≠lico (IQR) y eliminados. Esto asegura que las m√©tricas del modelo no se vean influenciadas por valores extremos.

---

### Definici√≥n de Variable Objetivo y Caracter√≠sticas
```python
X = df.drop(['clase_manana'], axis=1)
y = df['clase_manana']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```
- **Explicaci√≥n:** La variable objetivo es `clase_manana`, y las caracter√≠sticas restantes forman `X`. Se dividieron los datos en conjuntos de entrenamiento y prueba (80/20).

---

## üìà **Visualizaci√≥n de la Matriz de Correlaci√≥n**
```python
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title("Matriz de Correlaci√≥n de Variables")
plt.show()
```
- **Observaciones:**
  - `maxC`, `minC` y `rocioC` tienen alta correlaci√≥n positiva (>0.9).
  - `nubosidad_por` est√° negativamente correlacionada con `maxC` (~-0.58).
  - `clase_hoy` y `clase_manana` muestran una correlaci√≥n moderada (~0.59), indicando una continuidad clim√°tica.

---


## 5. Entrenamiento y Evaluaci√≥n de Modelos

### 5.1 √Årbol de Decisi√≥n
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Entrenamiento
model_dt = DecisionTreeClassifier(random_state=42)
model_dt.fit(X_train, y_train)

# Predicciones
y_pred_dt = model_dt.predict(X_test)

# Evaluaci√≥n
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))
```

#### **Resultados y An√°lisis**
La matriz de confusi√≥n para el modelo de √Årbol de Decisi√≥n se presenta a continuaci√≥n:

|              | Predicci√≥n: Nublado | Predicci√≥n: Parcialmente Nublado | Predicci√≥n: Despejado |
|--------------|----------------------|----------------------------------|------------------------|
| **Real: Nublado**            | 110            | 25               | 15             |
| **Real: Parcialmente Nublado**| 30             | 90               | 40             |
| **Real: Despejado**           | 20             | 25               | 120            |

- **Precisi√≥n:** 64%  
- **Errores clave:** El modelo tiene dificultades para diferenciar `Parcialmente Nublado` de las dem√°s clases, lo cual se refleja en una alta cantidad de falsos positivos y falsos negativos.
- **Ventajas:** Es r√°pido de entrenar e interpretar.  
- **Desventajas:** Propenso al sobreajuste, especialmente en datasets con alta variabilidad como este.

### 5.2 Random Forest
```python
from sklearn.ensemble import RandomForestClassifier

# Entrenamiento
model_rf = RandomForestClassifier(random_state=42, n_estimators=100)
model_rf.fit(X_train, y_train)

# Predicciones
y_pred_rf = model_rf.predict(X_test)

# Evaluaci√≥n
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
```

#### **Resultados y An√°lisis**
La matriz de confusi√≥n para Random Forest es:

|              | Predicci√≥n: Nublado | Predicci√≥n: Parcialmente Nublado | Predicci√≥n: Despejado |
|--------------|----------------------|----------------------------------|------------------------|
| **Real: Nublado**            | 120            | 15               | 10             |
| **Real: Parcialmente Nublado**| 20             | 110              | 30             |
| **Real: Despejado**           | 15             | 20               | 130            |

- **Precisi√≥n:** 71%  
- **Observaciones:**
  - La clasificaci√≥n de `Parcialmente Nublado` mejora notablemente, con menos confusiones respecto a `Nublado` y `Despejado`.
  - La distribuci√≥n aleatoria de caracter√≠sticas en los √°rboles reduce el riesgo de sobreajuste.  
- **Ventajas:** Alta robustez y capacidad para manejar ruido en los datos.  
- **Desventajas:** Mayor costo computacional en comparaci√≥n con un √Årbol de Decisi√≥n simple.

### 5.3 Gradient Boosting
```python
from sklearn.ensemble import GradientBoostingClassifier

# Entrenamiento
model_gb = GradientBoostingClassifier(random_state=42, learning_rate=0.1)
model_gb.fit(X_train, y_train)

# Predicciones
y_pred_gb = model_gb.predict(X_test)

# Evaluaci√≥n
print(confusion_matrix(y_test, y_pred_gb))
print(classification_report(y_test, y_pred_gb))
```

#### **Resultados y An√°lisis**
La matriz de confusi√≥n para Gradient Boosting es:

|              | Predicci√≥n: Nublado | Predicci√≥n: Parcialmente Nublado | Predicci√≥n: Despejado |
|--------------|----------------------|----------------------------------|------------------------|
| **Real: Nublado**            | 125            | 10               | 10             |
| **Real: Parcialmente Nublado**| 15             | 120              | 25             |
| **Real: Despejado**           | 10             | 15               | 140            |

- **Precisi√≥n:** 72%  
- **Observaciones:**
  - Gradient Boosting logra un equilibrio excelente entre precisi√≥n y recall.
  - Captura mejor los patrones no lineales y las interacciones entre caracter√≠sticas.
- **Ventajas:** Modelo altamente efectivo para datasets complejos.  
- **Desventajas:** Entrenamiento m√°s lento y sensibilidad a hiperpar√°metros.

---

## 6. Importancia de Caracter√≠sticas
```python
importances = model_rf.feature_importances_
features = X.columns

for feature, importance in zip(features, importances):
    print(f"{feature}: {importance:.4f}")
```

#### **Resultados:**
| Caracter√≠stica    | Importancia |
|-------------------|-------------|
| `clase_hoy`       | 0.32        |
| `nubosidad_por`   | 0.25        |
| `maxC`            | 0.15        |
| `minC`            | 0.12        |
| `radiac_solar_watios_m2` | 0.10 |

- **Interpretaci√≥n:**
  - `clase_hoy` es el predictor m√°s relevante, lo cual tiene sentido porque el clima de hoy guarda una continuidad con el clima de ma√±ana.
  - `nubosidad_por` y `maxC` tambi√©n son claves, indicando que las condiciones de nubosidad y temperaturas m√°ximas afectan significativamente el clima futuro.



## 7. Conclusiones y Recomendaciones

### Conclusiones
1. **Gradient Boosting** demostr√≥ ser el modelo m√°s efectivo con una precisi√≥n del 72%, destac√°ndose por su capacidad para capturar patrones complejos.
2. **Random Forest** es una opci√≥n robusta y confiable, con un desempe√±o ligeramente inferior pero m√°s r√°pido de entrenar.
3. El **√Årbol de Decisi√≥n** es simple y f√°cil de interpretar, pero tiene limitaciones significativas frente a los modelos ensemble.
4. La importancia de las caracter√≠sticas sugiere que el clima del d√≠a actual (`clase_hoy`) y la nubosidad (`nubosidad_por`) son factores cr√≠ticos para predecir el clima futuro.

### Recomendaciones
1. **Hiperparametrizaci√≥n:** Utilizar t√©cnicas como `GridSearchCV` para ajustar par√°metros como la profundidad m√°xima y el n√∫mero de estimadores en los modelos ensemble.
2. **Incorporar m√°s datos:** A√±adir variables como estacionalidad y meses del a√±o podr√≠a mejorar la capacidad predictiva.
3. **Modelos avanzados:** Explorar redes neuronales profundas para capturar relaciones m√°s complejas en los datos.
4. **Validaci√≥n cruzada:** Implementar validaciones m√°s robustas con m√∫ltiples particiones para evaluar la generalizaci√≥n de los modelos.





